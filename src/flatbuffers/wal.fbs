// Copyright 2017 Alexander Gallego
//
include "timestamp.fbs";
namespace smf.wal;

// TAG: alpha

enum wal_entry_compression_type:byte {
  none,
  zstd
}

struct wal_header {
  compression: wal_entry_compression_type = none;
  size:        uint = 0;
  checksum:    uint = 0;
}

struct wal_invalid_entry {
  invalid_entry: ulong;
}

enum tx_put_operation:byte {
  begin,
  data,
  commit,
  abort,

  /// \brief either the end of the txn OR a full transaction that is a small payload
  /// i.e.: fits in one request
  full
}

enum tx_put_invalidation_reason:byte {
  server_connection_timeout,
  client_connection_timeout
}
table tx_put_invalidation {
  reason: tx_put_invalidation_reason;
  /// iff this->op == tx_put_operation::abort
  /// use to write an abort, for a specific offset
  /// This is the offset from the tx_put_reply.
  offset: ulong;
}
table tx_put_kv {
  key:              [ubyte];
  value:            [ubyte];
}

union tx_put_data { tx_put_kv, tx_put_invalidation}
/// This is the datastructure that gets persisted on disk.
/// Please be tender while extending
table tx_put_fragment {
  /// what should we do with this trasaction fragment
  op:           tx_put_operation;
  /// sequence id
  client_seq:   ulong;
  /// In seastar clients we will use a losely ticked timer which moves
  /// every 10 micros.
  /// This is needed for systems doing window aggregations, etc.
  ///
  epoch:        smf.timestamp;
  data:         tx_put_data;
}

table tx_put_partition_pair {
  /// hashing_utils::xxhash_32( topic +  tx_put_fragment::data::key )
  partition:   uint (key); 

  // 
  // TODO(agallego) - optimize to a union - so that the client may be allowed to send bytes
  // requires a bit more sophisticated client. but probably worth it.
  //

  /// \brief - this could be optimized further if we make the client send bytes instead.
  /// currently kafka and other brokers do many re-compression
  ///   1) form the transport RPC mechanism
  ///   2) from the disk format and ultimate disk layout
  /// There is also at least one memcpy of all bytes into the caches:
  ///   1) To the disk cache - we keep a 1MB write-behind cache if the direct io
  ///      is catching behind
  ///   2) To the internal - page cache - since we don't use the operating system
  ///      which must keep 1MB per open file.
  ///
  /// We have locks, and many other mem copies the java.nio.channels.FileChannel
  /// uses for example https://goo.gl/hrTnjY
  /// since we don't then again copy into the page cache.
  ///
  /// Last, we don't keep any logs. Currently the kafka path is synchronized 3 times.
  /// 1) To get the leader & append the writes for the log section.
  /// 1.1) Roll the file - which happens under a relatively big critical region inside Log.scala
  /// 2) Into the operating system page cache
  /// 3) Into the java nio channel FileChannel since it 'guarantees' concurrent friendly updates
  ///
  txs:         [tx_put_fragment];
}

/// brief - stores `puts` transactionally
table tx_put_request {
  topic:   string;
  data:    [tx_put_partition_pair];
}

table tx_put_reply {
  /// \brief the committed offset into the WAL
  offset: ulong;
}


table tx_get_request {
  topic:      string;
  partition:  uint;
  offset:     ulong;
  /// (1 << 31) - 1 Max payload payload by flatbuffers 2GB-1
  /// we decrease it by 100 bytes so we can stuff headers in there
  /// plus plenty of room for growth
  ///
  /// >>> (2**31)-100
  /// 2147483548
  ///
  max_bytes:  uint = 2147483548;
}

/// \brief the broker might have decided to compress the tx_put_fragment.
/// 
table tx_get_fragment {
  hdr:             wal_header;
  /// \brief nothing more than a byte array of a possibly compressed
  /// tx_put_fragment
  fragment:        [ubyte]; 
}

table tx_get_reply {
  next_offset: ulong;
  gets:        [tx_get_fragment];
}

